# @package _global_

defaults:
  # override config either in this file or using experiment config
  - _self_
  - experiment: null # experiment configs allow for version control of specific hyperparameters
  - trainer: default.yaml
  - logger: tensorboard.yaml



# Setting
output_directory: "Pretraining"
deepspeed_config_path: "ds_config.json"

# Trainer
seed: 37
resume: false 
#resume_checkpoint: "Chemformer_Pretrain/mask_aug/version_1/checkpoints/step=100000.ckpt"
batch_size: 16
n_epochs: 100
limit_val_batches: 1.0
n_gpus: 1
n_nodes: 1
acc_batches: 1
accelerator: null 
check_val_every_n_epoch: 1

# Data
data_path: B3DBsim_external_test.tsv   # Required #null
vocabulary_path: bart_vocab_downstream.json
dataset_type: ""        # [chembl, zinc]
task: mask_aug              # [mask_aug]
mask_scheme: span            # ["span", "replace"]
mask_prob: 0.10
augmentation_probability: 0.0
data_device: cuda
n_buckets: 12

# Model
model_path:   
model_type: classifier            # ["bart", "unified"]
learning_rate: 5e-6 
max_seq_len: 512
d_model: 512
n_layers: 6
n_heads: 8
d_feedforward: 2048
activation: "gelu"
train_tokens: null
weight_decay: 0.0
clip_grad: 1.0
schedule: cycle
warm_up_steps: 50
train_mode: training
n_beams: 1


callbacks:
  - LearningRateMonitor
  - ModelCheckpoint:
    - period: 1
    - monitor: val_loss
  #- MetricsCallback
  - StepCheckpoint:
    - step_interval: 50000
  - ValidationScoreCallback
  - OptLRMonitor