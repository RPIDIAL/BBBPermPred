{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd6006c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0d286ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=float)\n",
    "    ty = np.empty([k, n], dtype=float)\n",
    "    tz = np.empty([k, m + n], dtype=float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    print(f'AUC model 1: {aucs[0]}, AUC model 2: {aucs[-1]}')\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    return order, label_1_count\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return calc_pvalue(aucs, delongcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_preds = pd.read_csv(\"/fast/grantn2/BBBPermeability/analysis_outputs/all_preds_long_format.csv\")\n",
    "all_preds = pd.read_csv(\"Outputs_Sept2025/all_preds_long_format.csv\")\n",
    "    # col1 \"trained_on\" --> [B3DB, NeuTox]\n",
    "    # col2 \"test_set\" --> [B3DB, NeuTox, BBBP]\n",
    "    # col3 \"feature_type\" --> [Descriptors, Embeddings, Combined]\n",
    "    # col4 \"fold\" --> [0, 1, 2, 3, 4]\n",
    "    # col5 \"classifier\" --> [RF, SVM, LR, MLP]\n",
    "    # col6 \"label\"\n",
    "    # col7 \"prob\"\n",
    "    # col8 \"pred\"\n",
    "ensemble_preds = pd.read_csv(\"Outputs_Sept2025/all_preds_long_format.csv\")\n",
    "# Comparisons I want to do: keep trained_on and test_set the same across all comparisons\n",
    "# Our model ---> Classifer == MLP & feature_type == Combined\n",
    "\n",
    "# 1. Combined features vs Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b901fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = \"B3DB\"\n",
    "test_set = \"B3DB\" \n",
    "\n",
    "\n",
    "# Base Model: MLP + Combined features\n",
    "base_model = all_preds[(all_preds[\"trained_on\"] == training_set) & (all_preds[\"test_set\"] == test_set) & (all_preds[\"feature_type\"] == \"Combined\") & (all_preds[\"classifier\"] == \"MLP\")]\n",
    "base_model_labels = base_model[\"label\"].values\n",
    "base_model_probs = base_model[\"prob\"].values\n",
    "\n",
    "# Descriptors Model: MLP + Descriptors\n",
    "desc_model = all_preds[(all_preds[\"trained_on\"] == training_set) & (all_preds[\"test_set\"] == test_set) & (all_preds[\"feature_type\"] == \"Descriptors\") & (all_preds[\"classifier\"] == \"MLP\")]\n",
    "desc_model_probs = desc_model[\"prob\"].values\n",
    "desc_model_labels = desc_model[\"label\"].values\n",
    "\n",
    "# Encoded Feat Model: MLP + Encoded Features\n",
    "encoded_model = all_preds[(all_preds[\"trained_on\"] == training_set) & (all_preds[\"test_set\"] == test_set) & (all_preds[\"feature_type\"] == \"Embeddings\") & (all_preds[\"classifier\"] == \"MLP\")]\n",
    "encoded_model_probs = encoded_model[\"prob\"].values\n",
    "encoded_model_labels = encoded_model[\"label\"].values\n",
    "\n",
    "# SVM Model: SVM + Combined features\n",
    "svm_model = all_preds[(all_preds[\"trained_on\"] == training_set) & (all_preds[\"test_set\"] == test_set) & (all_preds[\"feature_type\"] == \"Combined\") & (all_preds[\"classifier\"] == \"SVM\")]\n",
    "svm_model_probs = svm_model[\"prob\"].values\n",
    "svm_model_labels = svm_model[\"label\"].values\n",
    "\n",
    "# Logistic Regression Model: LR + Combined features\n",
    "lr_model = all_preds[(all_preds[\"trained_on\"] == training_set) & (all_preds[\"test_set\"] == test_set) & (all_preds[\"feature_type\"] == \"Combined\") & (all_preds[\"classifier\"] == \"LR\")]\n",
    "lr_model_probs = lr_model[\"prob\"].values\n",
    "lr_model_labels = lr_model[\"label\"].values\n",
    "\n",
    "# RF Model: RF + Combined features\n",
    "rf_model = all_preds[(all_preds[\"trained_on\"] == training_set) & (all_preds[\"test_set\"] == test_set) & (all_preds[\"feature_type\"] == \"Combined\") & (all_preds[\"classifier\"] == \"RF\")]\n",
    "rf_model_probs = rf_model[\"prob\"].values\n",
    "rf_model_labels = rf_model[\"label\"].values\n",
    "\n",
    "# Multi-Head Model: Desc head, encoder head, combined head\n",
    "mh_model = ensemble_preds[(ensemble_preds[\"trained_on\"] == training_set) & (ensemble_preds[\"test_set\"] == test_set) & (ensemble_preds[\"feature_type\"] == \"All\") & (ensemble_preds[\"classifier\"] == \"Ensemble\")]\n",
    "mh_model_probs = mh_model[\"prob\"].values\n",
    "mh_model_labels = mh_model[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5be83571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that labels are the same across all models\n",
    "assert np.array_equal(base_model_labels, desc_model_labels)\n",
    "assert np.array_equal(base_model_labels, encoded_model_labels)\n",
    "assert np.array_equal(base_model_labels, svm_model_labels)\n",
    "assert np.array_equal(base_model_labels, lr_model_labels)\n",
    "assert np.array_equal(base_model_labels, rf_model_labels)\n",
    "assert np.array_equal(base_model_labels, mh_model_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf73c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that labels are the same across all models\n",
    "\n",
    "\n",
    "# Compare everything to the mH_model\n",
    "\n",
    "assert np.array_equal(mh_model_labels, desc_model_labels)\n",
    "assert np.array_equal(mh_model_labels, encoded_model_labels)\n",
    "assert np.array_equal(mh_model_labels, svm_model_labels)\n",
    "assert np.array_equal(mh_model_labels, lr_model_labels)\n",
    "assert np.array_equal(mh_model_labels, rf_model_labels)\n",
    "assert np.array_equal(mh_model_labels, base_model_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1706d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9602921340608424\n",
      "Comparison: MLP Ensemble (Base Model) vs Descriptors Model\n",
      "Log p-value: 0.000\n",
      "-\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9856743050051819\n",
      "Comparison: MLP Ensemble (Base Model) vs Encoded Features Model\n",
      "Log p-value: 0.520\n",
      "-\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9396394714381515\n",
      "Comparison: MLP Ensemble (Base Model) vs SVM Model\n",
      "Log p-value: 0.000\n",
      "-\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9479877918673414\n",
      "Comparison: MLP Ensemble (Base Model) vs Logistic Regression Model\n",
      "Log p-value: 0.000\n",
      "-\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9919193516429918\n",
      "Comparison: MLP Ensemble (Base Model) vs RF Model\n",
      "Log p-value: 0.019\n",
      "-\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9850894272389199\n",
      "Comparison: MLP Ensemble (Base Model) vs Combined Model\n",
      "Log p-value: 0.204\n",
      "-\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9602921340608424\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9856743050051819\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9396394714381515\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9479877918673414\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9919193516429918\n",
      "AUC model 1: 0.9868697799183075, AUC model 2: 0.9850894272389199\n"
     ]
    }
   ],
   "source": [
    "Models = {\n",
    "    \"Descriptors Model\": desc_model_probs,\n",
    "    \"Encoded Features Model\": encoded_model_probs,\n",
    "    \"SVM Model\": svm_model_probs,\n",
    "    \"Logistic Regression Model\": lr_model_probs,\n",
    "    \"RF Model\": rf_model_probs,\n",
    "    \"Combined Model\": base_model_probs\n",
    "}\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "output_path = \"/fast/grantn2/BBBPermeability/analysis_outputs/DeLong_test_results/Oct2025\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "output_file = os.path.join(output_path, f\"DeLong_train_{training_set}_test_{test_set}.csv\")\n",
    "\n",
    "\n",
    "for model_name, model_predictions in Models.items():\n",
    "    # Perform DeLong test\n",
    "    log_p_value = delong_roc_test(mh_model_labels, mh_model_probs, model_predictions)\n",
    "\n",
    "    # Convert log p-value to standard p-value\n",
    "    log_p_value = np.exp(np.log(10)*log_p_value)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Comparison: MLP Ensemble (Base Model) vs {model_name}\")\n",
    "    print(f\"Log p-value: {log_p_value.item():.3f}\")\n",
    "    # print(f\"p-value: {p_value.item():.3e}\")\n",
    "    print(\"-\")\n",
    "\n",
    "\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Comparison\", \"Log p-value\"])\n",
    "    for model_name, model_predictions in Models.items():\n",
    "        log_p_value = delong_roc_test(mh_model_labels, mh_model_probs, model_predictions)\n",
    "        log_p_value = np.exp(np.log(10)*log_p_value)\n",
    "        writer.writerow([f\"MLP Ensemble (Base Model) vs {model_name}\", log_p_value.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18713cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making ROC curves for each\n",
    "#for model_name, model_probs in Models.items():\n",
    "#    from sklearn.metrics import roc_curve, auc\n",
    "##    import matplotlib.pyplot as plt\n",
    "\n",
    " #   fpr, tpr, _ = roc_curve(base_model_labels, model_probs)\n",
    "  ##\n",
    "   ## plt.figure()\n",
    "    #plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "   # plt.xlabel('False Positive Rate')\n",
    "   # plt.ylabel('True Positive Rate')\n",
    "   # plt.title(f'Receiver Operating Characteristic - {model_name}')\n",
    "   # plt.legend(loc=\"lower right\")\n",
    "   # plt.savefig(f\"/fast/grantn2/BBBPermeability/Outputs_Sept2025/ROC_Curve/{training_set}_train_{test_set}_{model_name.replace(' ', '_')}.png\")\n",
    "   # plt.show()\n",
    "   #plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
